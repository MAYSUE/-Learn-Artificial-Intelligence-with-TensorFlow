{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Can-a-Neural-Network-Fit-Random-Data?\" data-toc-modified-id=\"Can-a-Neural-Network-Fit-Random-Data?-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Can a Neural Network Fit Random Data?</a></div><div class=\"lev2 toc-item\"><a href=\"#Training-and-Saving\" data-toc-modified-id=\"Training-and-Saving-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Training and Saving</a></div><div class=\"lev2 toc-item\"><a href=\"#Loading\" data-toc-modified-id=\"Loading-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Loading</a></div><div class=\"lev2 toc-item\"><a href=\"#Old-School-Saving-and-Loading\" data-toc-modified-id=\"Old-School-Saving-and-Loading-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Old School Saving and Loading</a></div><div class=\"lev1 toc-item\"><a href=\"#My-Notes\" data-toc-modified-id=\"My-Notes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>My Notes</a></div><div class=\"lev2 toc-item\"><a href=\"#Unsolved-Mysteries\" data-toc-modified-id=\"Unsolved-Mysteries-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Unsolved Mysteries</a></div><div class=\"lev3 toc-item\"><a href=\"#DISAMBIGUATING-GRAPH,-METAGRAPH,-VARIABLES,-OPS,-AND-MORE\" data-toc-modified-id=\"DISAMBIGUATING-GRAPH,-METAGRAPH,-VARIABLES,-OPS,-AND-MORE-211\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>DISAMBIGUATING GRAPH, METAGRAPH, VARIABLES, OPS, AND MORE</a></div><div class=\"lev2 toc-item\"><a href=\"#SavedModelBuilder\" data-toc-modified-id=\"SavedModelBuilder-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>SavedModelBuilder</a></div><div class=\"lev3 toc-item\"><a href=\"#Trimmed-Impl-of-SavedModelBuilder\" data-toc-modified-id=\"Trimmed-Impl-of-SavedModelBuilder-221\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Trimmed Impl of SavedModelBuilder</a></div><div class=\"lev2 toc-item\"><a href=\"#tf.train.Saver\" data-toc-modified-id=\"tf.train.Saver-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>tf.train.Saver</a></div><div class=\"lev2 toc-item\"><a href=\"#Misc\" data-toc-modified-id=\"Misc-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Misc</a></div><div class=\"lev1 toc-item\"><a href=\"#Copy-Pastes-for-Plane-Ride\" data-toc-modified-id=\"Copy-Pastes-for-Plane-Ride-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Copy-Pastes for Plane Ride</a></div><div class=\"lev2 toc-item\"><a href=\"#SavedModelBuilder\" data-toc-modified-id=\"SavedModelBuilder-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>SavedModelBuilder</a></div><div class=\"lev2 toc-item\"><a href=\"#Loader\" data-toc-modified-id=\"Loader-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Loader</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to mnist.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can a Neural Network Fit Random Data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "num_features = 28\n",
    "num_classes = 5\n",
    "\n",
    "batch_size = 32\n",
    "hidden_size = 64\n",
    "\n",
    "num_batches = 50\n",
    "fake_inputs = np.random.random(size=(num_batches, batch_size, num_features))\n",
    "fake_labels = np.random.randint(0, num_classes, size=(num_batches, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf.placeholder for values we will set in sess.run() calls. \n",
    "input_layer = tf.placeholder(\n",
    "    dtype=tf.float32, shape=(None, num_features), name='input_layer')\n",
    "# Define the two layers with ReLU activation functions.\n",
    "hidden_layer_1 = tf.layers.dense(\n",
    "    inputs=input_layer, units=hidden_size, activation=tf.nn.relu, name='hidden_layer_1')\n",
    "hidden_layer_2 = tf.layers.dense(\n",
    "    inputs=hidden_layer_1, units=hidden_size, activation=tf.nn.relu, name='hideen_layer_2')\n",
    "# Project to output layer of dimensionality equal to the number of classes to predict.\n",
    "# This layer is often called the \"logits\". \n",
    "output_layer = tf.layers.dense(\n",
    "    inputs=hidden_layer_2, units=num_classes, name='output_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our loss function and training operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = tf.placeholder(dtype=tf.int32, shape=(None,), name='labels')\n",
    "onehot_labels = tf.one_hot(labels, num_classes, name='onehot_labels')\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(\n",
    "    onehot_labels=onehot_labels,\n",
    "    logits=output_layer)\n",
    "global_step = tf.get_variable(\n",
    "    'global_step', shape=(), dtype=tf.int32, trainable=False)\n",
    "train_op = tf.contrib.layers.optimize_loss(\n",
    "    loss=loss,\n",
    "    global_step=global_step,\n",
    "    learning_rate=0.0001,\n",
    "    optimizer='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate our model by computing the number of correct predictions it makes in a given batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = tf.nn.in_top_k(output_layer, labels, k=1)\n",
    "num_correct = tf.reduce_sum(tf.to_int32(correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place the objects we want to use in future `sess.run` calls in collections, so that we can easily access them upon loading models from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.add_to_collection('fetches', num_correct)\n",
    "tf.add_to_collection('fetches', train_op)\n",
    "\n",
    "tf.add_to_collection('feed_dict', input_layer)\n",
    "tf.add_to_collection('feed_dict', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: accuracy=20.125%\n",
      "Epoch 100: accuracy=35.062%\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'out/saved_model.pb'\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "export_dir = 'out'\n",
    "\n",
    "def run_training_epoch(sess):\n",
    "    input_layer, labels = tf.get_collection('feed_dict')\n",
    "    num_correct_total = 0\n",
    "    for batch_idx in range(num_batches):\n",
    "        outputs = sess.run(tf.get_collection('fetches'), feed_dict={\n",
    "            input_layer: fake_inputs[batch_idx],\n",
    "            labels: fake_labels[batch_idx]})\n",
    "        num_correct_total += outputs[0]\n",
    "    return num_correct_total\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_idx in range(num_epochs):\n",
    "        num_correct_total = run_training_epoch(sess)\n",
    "        if epoch_idx % 100 == 0:\n",
    "            print('Epoch {}: accuracy={:.3%}'.format(\n",
    "                epoch_idx, float(num_correct_total) / (num_batches * batch_size)))\n",
    "            \n",
    "    builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "    builder.add_meta_graph_and_variables(\n",
    "        sess=sess,\n",
    "        tags=[tf.saved_model.tag_constants.TRAINING])\n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[loader.py](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/saved_model/loader.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'out/variables/variables'\n",
      "Epoch 0: accuracy=41.125%\n",
      "Epoch 100: accuracy=47.188%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.saved_model.loader.load(\n",
    "        sess=sess,\n",
    "        tags=[tf.saved_model.tag_constants.TRAINING], \n",
    "        export_dir=export_dir)\n",
    "    \n",
    "    for epoch_idx in range(num_epochs):\n",
    "        num_correct_total = run_training_epoch(sess)\n",
    "        if epoch_idx % 100 == 0:\n",
    "            print('Epoch {}: accuracy={:.3%}'.format(epoch_idx, float(num_correct_total) / (num_batches * batch_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old School Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    save_path = saver.save(sess, 'out/0/saver.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "v1 = tf.get_variable('v1', shape=[3]) # create ur graph as usual\n",
    "saver = tf.train.Saver() # even create saver as usual yo!\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '/tmp/model.ckpt') # oh shit whaddup\n",
    "    # check ur vars breh\n",
    "    print('v1 :', v1.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tf.train.import_meta_graph](https://www.tensorflow.org/api_docs/python/tf/train/import_meta_graph)\n",
    "\n",
    "Recreates a Graph saved in a MetaGraphDef proto.\n",
    "\n",
    "This function takes a MetaGraphDef protocol buffer as input. If the argument is a file containing a MetaGraphDef protocol buffer , it constructs a protocol buffer from the file content. The function then adds all the nodes from the graph_def field to the current graph, recreates all the collections, and returns a saver constructed from the saver_def field.\n",
    "\n",
    "In combination with export_meta_graph(), this function can be used to\n",
    "\n",
    "Serialize a graph along with other Python objects such as QueueRunner, Variable into a MetaGraphDef.\n",
    "Restart training from a saved graph and checkpoints.\n",
    "Run inference from a saved graph and checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "...\n",
    "# Create a saver.\n",
    "saver = tf.train.Saver(...variables...)\n",
    "# Remember the training_op we want to run by adding it to a collection.\n",
    "tf.add_to_collection('train_op', train_op)\n",
    "sess = tf.Session()\n",
    "for step in xrange(1000000):\n",
    "    sess.run(train_op)\n",
    "    if step % 1000 == 0:\n",
    "        # Saves checkpoint, which by default also exports a meta_graph\n",
    "        # named 'my-model-global_step.meta'.\n",
    "        saver.save(sess, 'my-model', global_step=step)\n",
    "        \n",
    "# TWO....DAYS...LATER...\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')\n",
    "  new_saver.restore(sess, 'my-save-dir/my-model-10000')\n",
    "  # tf.get_collection() returns a list. In this example we only want the\n",
    "  # first one.\n",
    "  train_op = tf.get_collection('train_op')[0]\n",
    "  for step in xrange(1000000):\n",
    "    sess.run(train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Restarting training from saved meta_graph only works if the device assignments have not changed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsolved Mysteries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When you want to save and load variables, the graph, and the graph's metadata.\n",
    "\n",
    "There is some redundancy/sloppiness going on in that sentence, right? What's the difference between each of those exactly? Ok, it is about time I dive into this rabbit hole:\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "### DISAMBIGUATING GRAPH, METAGRAPH, VARIABLES, OPS, AND MORE\n",
    "\n",
    "__tf.Graph__: \"A Graph contains a set of \n",
    "- tf.Operation objects, which represent units of computation\n",
    "- tf.Tensor objects, which represent the units of data that flow between operations.\"\n",
    "\n",
    "Ok if you actually read the code (and protos), you find that, more technically, _a Graph is a set of Nodes, and a Node is mainly defined by an Operation and the input tensor names for that operation._\n",
    "\n",
    "OK I THINK I GET IT NOW (after reading protos below): A SavedModelBuilder literally saves a list of MetaGraphDefs, each of which specifies:\n",
    "1. A GraphDef. This is literally a list of NodeDefs, each of which specify a tf.Operation and a set of tf.Tensors that are fed as input to the operation. Notice how this is purely _structural_ and isn't concerned at all with particular values for any of those tensors (CRUCIAL TO UNDERSTAND THAT SENTENCE). \n",
    "2. A SaverDef. Tells how to save and restore __variables__. This is how we get access to the particular values of tensors in our graph (via their corresponding tf.Variable). \n",
    "3. A {string => CollectionDef} map. \n",
    "4. A {string => SignatureDef} map. A SignatureDef contains two {string => TensorInfo} maps, one for inputs and one for outputs.\n",
    "5. A list of AssetFileDefs.\n",
    "\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SavedModelBuilder\n",
    "\n",
    "- [saved_model.proto](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/core/protobuf/saved_model.proto)\n",
    "- [model_builder_impl.py](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/saved_model/builder_impl.py#L39)\n",
    "\n",
    "\n",
    "```c\n",
    "message SavedModel {\n",
    "    // CTor just inits this to tf.saved_model.constants.SAVED_MODEL_SCHEMA_VERSION\n",
    "    int64 saved_model_schema_version = 1;\n",
    "    repeated MetaGraphDef meta_graphs = 2;\n",
    "}\n",
    "\n",
    "message MetaGraphDef {\n",
    "    MetaInfoDef meta_info_def = 1;\n",
    "    \n",
    "    // GRAPH DEF AND NODE DEF\n",
    "    message GraphDef {\n",
    "        message NodeDef {\n",
    "            string name = 1;\n",
    "            string op = 2;\n",
    "            repeated string input = 3;\n",
    "            string device = 4;\n",
    "            map<string, AttrValue> attr = 5;\n",
    "        }\n",
    "        repeated NodeDef node = 1;\n",
    "        VersionDef versions = 2;\n",
    "    }\n",
    "    GraphDef graph_def = 2;\n",
    "    \n",
    "    // SAVER DEF\n",
    "    message SaverDef {\n",
    "        string filename_tensor_name = 1;\n",
    "        string save_tensor_name = 2; // saving Operation()\n",
    "        string restore_op_name = 3;\n",
    "        int32 max_to_keep = 4;\n",
    "        bool sharded = 5;\n",
    "        float keep_checkpoint_ever_n_hours = 6;\n",
    "    }\n",
    "    SaverDef saver_def = 3;\n",
    "\n",
    "    // COLLECTION DEF\n",
    "    message CollectionDef {\n",
    "        message NodeList { repeated string value = 1; }\n",
    "        message BytesList { repeated bytes value = 1; }\n",
    "        message Int64List { repeated int64 value = 1 [packed = true]; }\n",
    "        message FloatList { repeated float value = 1 [packed = true]; }\n",
    "        message AnyList { repeated google.protobuf.Any value = 1; }\n",
    "        \n",
    "        oneof kind {\n",
    "            NodeList node_list = 1;\n",
    "            BytesList bytes_list = 2;\n",
    "            Int64List int64_list = 3;\n",
    "            FloatList float_list = 4;\n",
    "            AnyList any_list = 5;\n",
    "        }\n",
    "    }\n",
    "    map<string, CollectionDef> collection_def = 4;\n",
    "    \n",
    "    // SIGNATURE DEF\n",
    "    message SignatureDef {\n",
    "        map<string, TensorInfo> inputs = 1;\n",
    "        map<string, TensorInfo> outputs = 2;\n",
    "    }\n",
    "    map<string, SignatureDef> signature_def = 5;\n",
    "    \n",
    "    // ASSET FILE DEF AND TENSOR INFO\n",
    "    message AssetFileDef {\n",
    "        message TensorInfo {\n",
    "            message CooSparse { values_tensor_name, indices_tensor_name, dense_shape_tensor_name } // pseudo-proto\n",
    "            oneof encoding {\n",
    "                string name; // For dense Tensors\n",
    "                CooSparse coo_sparse; // COO encoding for sparse tensors\n",
    "            }\n",
    "            DateType dtype;\n",
    "            TensorShapeProto tensor_shape;\n",
    "        }\n",
    "        TensorInfo tensor_info = 1;\n",
    "        string filename = 2;\n",
    "    }\n",
    "    repeated AssetFileDef asset_file_def = 6;\n",
    "}\n",
    "\n",
    "// ------------------------------------\n",
    "// GRAPH STUFF\n",
    "// -----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimmed Impl of SavedModelBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.core.protobuf import *\n",
    "from tensorflow.python.saved_model import constants\n",
    "\n",
    "class SavedModelBuilder:\n",
    "    \n",
    "    def __init__(self, export_dir):\n",
    "        self._saved_model = saved_model_pb2.SavedModel(\n",
    "            saved_model_schema_version=constants.SAVED_MODEL_SCHEMA_VERSION)\n",
    "        self._export_dir = export_dir # Real impl checks that it doesnt exist & then makes it.\n",
    "        \n",
    "    def add_meta_graph_and_variables(self, sess, tags,\n",
    "                                    signature_def_map=None,\n",
    "                                    assets_collections=None,\n",
    "                                    legacy_init_op=None,\n",
    "                                    clear_devices=False,\n",
    "                                    main_op=None):\n",
    "        \n",
    "        if main_op is not None: self._maybe_add_legacy_init_op(legacy_init_op)\n",
    "        else: self._add_main_op(main_op)\n",
    "            \n",
    "        saver = tf.train.Saver(\n",
    "            var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) + tf.get_collection(tf.GraphKeys.SAVEABLE_OBJECTS),\n",
    "            sharded=True, \n",
    "            write_version=saver_pb2.SaverDef.V2,\n",
    "            allow_empty=True)\n",
    "        \n",
    "        # Save the variables.\n",
    "        saver.save(sess, variables_path, write_meta_graph=False, write_state=False)\n",
    "        # Export the meta graph def.\n",
    "        meta_graph_def = saver.export_meta_graph_def(clear_devices=clear_devices)   \n",
    "        # Tag the meta graph def and add it to the SavedModel.\n",
    "        self._tag_and_add_meta_graph(meta_graph_def, tags, signature_def_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved_model_proto = saved_model_pb2.SavedModel()\n",
    "meta_graph_proto = meta_graph_pb2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.train.Saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [API Documentation](https://www.tensorflow.org/api_docs/python/tf/train/Saver)\n",
    "\n",
    "Saves and restores variables.\n",
    "\n",
    "The Saver class adds ops to save and restore variables to and from checkpoints. \n",
    "It also provides convenience methods to run these ops.\n",
    "\n",
    "Checkpoints are binary files in a proprietary format which map variable names to tensor values. \n",
    "The best way to examine the contents of a checkpoint is to load it using a Saver.\n",
    "\n",
    "Savers can automatically number checkpoint filenames with a provided counter. \n",
    "This lets you keep multiple checkpoints at different steps while training a model. \n",
    "For example you can number the checkpoint filenames with the training step number. \n",
    "To avoid filling up disks, savers manage checkpoint files automatically. \n",
    "For example, they can keep only the N most recent files, or one checkpoint for every N hours of training.\n",
    "\n",
    "You number checkpoint filenames by passing a value to the optional global_step argument to save():\n",
    "\n",
    "```python\n",
    "saver.save(sess, 'my-model', global_step=0) ==> filename: 'my-model-0'\n",
    "...\n",
    "saver.save(sess, 'my-model', global_step=1000) ==> filename: 'my-model-1000'\n",
    "```\n",
    "Additionally, optional arguments to the Saver() constructor let you control the proliferation of checkpoint files on disk:\n",
    "* max_to_keep indicates the maximum number of recent checkpoint files to keep. As new files are created, older files are deleted. If None or 0, all checkpoint files are kept. Defaults to 5 (that is, the 5 most recent checkpoint files are kept.)\n",
    "* keep_checkpoint_every_n_hours: In addition to keeping the most recent max_to_keep checkpoint files, you might want to keep one checkpoint file for every N hours of training. This can be useful if you want to later analyze how a model progressed during a long training session. For example, passing keep_checkpoint_every_n_hours=2 ensures that you keep one checkpoint file for every 2 hours of training. The default value of 10,000 hours effectively disables the feature.\n",
    "\n",
    "\n",
    "Note that you still have to call the save() method to save the model. Passing these arguments to the constructor will not save variables automatically for you.\n",
    "\n",
    "A training program that saves regularly looks like:\n",
    "\n",
    "```python\n",
    "# Create a saver.\n",
    "saver = tf.train.Saver(...variables...)\n",
    "# Launch the graph and train, saving the model every 1,000 steps.\n",
    "sess = tf.Session()\n",
    "for step in xrange(1000000):\n",
    "    sess.run(..training_op..)\n",
    "    if step % 1000 == 0:\n",
    "        # Append the step number to the checkpoint name:\n",
    "        saver.save(sess, 'my-model', global_step=step)\n",
    "```\n",
    "\n",
    "In addition to checkpoint files, savers keep a protocol buffer on disk with the list of recent checkpoints. This is used to manage numbered checkpoint files and by latest_checkpoint(), which makes it easy to discover the path to the most recent checkpoint. That protocol buffer is stored in a file named 'checkpoint' next to the checkpoint files.\n",
    "\n",
    "If you create several savers, you can specify a different filename for the protocol buffer file in the call to save().\n",
    "\n",
    "__init__ :\n",
    "```python\n",
    "__init__(\n",
    "    var_list=None,\n",
    "    reshape=False,\n",
    "    sharded=False,\n",
    "    max_to_keep=5,\n",
    "    keep_checkpoint_every_n_hours=10000.0,\n",
    "    name=None,\n",
    "    restore_sequentially=False,\n",
    "    saver_def=None,\n",
    "    builder=None,\n",
    "    defer_build=False,\n",
    "    allow_empty=False,\n",
    "    write_version=tf.train.SaverDef.V2,\n",
    "    pad_step_number=False,\n",
    "    save_relative_paths=False,\n",
    "    filename=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "REALLY USEFUL DESCRIPTION OF DEVICE NAME SEMANTICS IN [node_def.proto](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/core/framework/node_def.proto):\n",
    "\n",
    "```c\n",
    "  // A (possibly partial) specification for the device on which this\n",
    "  // node should be placed.\n",
    "  // The expected syntax for this string is as follows:\n",
    "  //\n",
    "  // DEVICE_SPEC ::= PARTIAL_SPEC\n",
    "  //\n",
    "  // PARTIAL_SPEC ::= (\"/\" CONSTRAINT) *\n",
    "  // CONSTRAINT ::= (\"job:\" JOB_NAME)\n",
    "  //              | (\"replica:\" [1-9][0-9]*)\n",
    "  //              | (\"task:\" [1-9][0-9]*)\n",
    "  //              | ( (\"gpu\" | \"cpu\") \":\" ([1-9][0-9]* | \"*\") )\n",
    "  //\n",
    "  // Valid values for this string include:\n",
    "  // * \"/job:worker/replica:0/task:1/device:GPU:3\"  (full specification)\n",
    "  // * \"/job:worker/device:GPU:3\"                   (partial specification)\n",
    "  // * \"\"                                    (no specification)\n",
    "  //\n",
    "  // If the constraints do not resolve to a single device (or if this\n",
    "  // field is empty or not present), the runtime will attempt to\n",
    "  // choose a device automatically.\n",
    "```\n",
    "\n",
    "Note: may be useful to see how Estimators save their shit..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Copy-Pastes for Plane Ride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "## SavedModelBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "export_dir = ...\n",
    "...\n",
    "builder = tf.saved_model_builder.SavedModelBuilder(export_dir)\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "  ...\n",
    "  builder.add_meta_graph_and_variables(sess,\n",
    "                                       [tag_constants.TRAINING],\n",
    "                                       signature_def_map=foo_signatures,\n",
    "                                       assets_collection=foo_assets)\n",
    "...\n",
    "# Add a second MetaGraphDef for inference.\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "  ...\n",
    "  builder.add_meta_graph([tag_constants.SERVING])\n",
    "...\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "export_dir = ...\n",
    "...\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "  tf.saved_model.loader.load(sess, [tag_constants.TRAINING], export_dir)\n",
    "  ..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "4px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "40px",
    "left": "1441px",
    "right": "20px",
    "top": "106px",
    "width": "213px"
   },
   "toc_section_display": "none",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
